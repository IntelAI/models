name: Check performance
author: gerardo.dominguez.aldama@intel.com
description: |
  This action compares the result of a workload test with a specified threshold
  and writes a GITHUB_SUMMARY to show if the workload has an accepted tolerance.

inputs:
  output_folder:
    description: Output folder were test results were saved.
    required: true
    default: ${GITHUB_WORKSPACE}/tests/cicd/output
  workload:
    description: Workload test.
    required: true
    default: resnet50v1.5-inference
  script:
    description: Script being executed by the workload.
    required: true
    default: inference_realtime_multi_instance.sh
  benchmark_type:
    description: Type of result to compare, eg. latency, throughput, accuracy, etc.
    required: true
    default: throughput
  precisions:
    description: List of precisions to analyze.
    required: true
    default: "['int8','fp32','bfloat16','bfloat32']"
  thresholds:
    description: List of thresholds to compare the results.
    required: true
    default: "['650','625','600','575']"
  tolerance:
    description: Specifies the accepted range where the result can exist.
    required: true
    default: 0.05

runs:
  using: 'composite'
  steps:
    - name: Install dependencies
      shell: bash
      run: |
        pip install pandas
    - name: Check performance
      shell: bash
      run: |
        mkdir ${{ inputs.output_folder }}/performances
        IFS=',()][' read -a PRECISIONS <<< ${{ inputs.precisions }}
        IFS=',()][' read -a THRESHOLDS <<< ${{ inputs.thresholds }}
        NUM_THRESHOLDS=${#THRESHOLDS[@]}
        for idx in $(seq 0 $NUM_THRESHOLDS); do
          if [ "${THRESHOLDS[idx]}" != '' ]; then
            LOWER_LIMIT=$(echo "${THRESHOLDS[idx]}*(1-${{ inputs.tolerance }})" | bc -l)
            UPPER_LIMIT=$(echo "${THRESHOLDS[idx]}*(1+${{ inputs.tolerance }})" | bc -l)
            cd ${{ inputs.output_folder }}/${{ env.FOLDER }}/${{ inputs.workload }}/${{ inputs.script }}/${PRECISIONS[idx]}
            OUTPUT_FILES=(*)
            NUM_OUTPUT_FILES=${#OUTPUT_FILES[@]}
            for idy in $(seq 0 $NUM_OUTPUT_FILES); do
              RESULT=$(grep -i ${{ inputs.benchmark_type }} ${OUTPUT_FILES[idy]} | grep -Eo '[0-9]+\.[0-9]+{1,4}' | tail -n 1) || echo "Benchmark not found on file ${idy}"
              if [[ ! -z "${RESULT}" ]]; then
                break
              fi
            done
            if [[ "${{ inputs.benchmark_type }}" == "accuracy" ]] && (( $(echo "${RESULT} > 1" | bc -l) )); then
              RESULT=$(grep -i ${{ inputs.benchmark_type }} ${OUTPUT_FILES[idy]} | grep -Eo '[0-9]+\.[0-9]+{1,4}' | tail -n 2 | head -1)
              if (( $(echo "${RESULT} > 1" | bc -l) )); then
                RESULT=0
              fi
            fi

            if [[ "${{ inputs.benchmark_type }}" == "latency" ]] && (( $(echo "${RESULT} < ${UPPER_LIMIT}" | bc -l) )); then
                python ${GITHUB_WORKSPACE}/.github/actions/check-performance/create-df.py ${{ env.FOLDER }} ${{ inputs.workload }} ${{ inputs.script }} ${PRECISIONS[idx]} ${{ inputs.benchmark_type }} ${{ inputs.tolerance }} ${THRESHOLDS[idx]} ${RESULT} PASS ${{ inputs.output_folder }}
            elif [[ "${{ inputs.benchmark_type }}" == "latency" ]] && (( $(echo "${RESULT} >= ${UPPER_LIMIT}" | bc -l) )); then
                python ${GITHUB_WORKSPACE}/.github/actions/check-performance/create-df.py ${{ env.FOLDER }} ${{ inputs.workload }} ${{ inputs.script }} ${PRECISIONS[idx]} ${{ inputs.benchmark_type }} ${{ inputs.tolerance }} ${THRESHOLDS[idx]} ${RESULT} FAIL ${{ inputs.output_folder }}
            elif [[ "${{ inputs.benchmark_type }}" == "throughput" || "${{ inputs.benchmark_type }}" == "accuracy" ]] && (( $(echo "${RESULT} > ${LOWER_LIMIT}" | bc -l) )); then
                python ${GITHUB_WORKSPACE}/.github/actions/check-performance/create-df.py ${{ env.FOLDER }} ${{ inputs.workload }} ${{ inputs.script }} ${PRECISIONS[idx]} ${{ inputs.benchmark_type }} ${{ inputs.tolerance }} ${THRESHOLDS[idx]} ${RESULT} PASS ${{ inputs.output_folder }}
            elif [[ "${{ inputs.benchmark_type }}" == "throughput" || "${{ inputs.benchmark_type }}" == "accuracy" ]] && (( $(echo "${RESULT} <= ${LOWER_LIMIT}" | bc -l) )); then
                python ${GITHUB_WORKSPACE}/.github/actions/check-performance/create-df.py ${{ env.FOLDER }} ${{ inputs.workload }} ${{ inputs.script }} ${PRECISIONS[idx]} ${{ inputs.benchmark_type }} ${{ inputs.tolerance }} ${THRESHOLDS[idx]} ${RESULT} FAIL ${{ inputs.output_folder }}
            fi
          fi
        done
    - uses: actions/upload-artifact@v3
      with:
        name: workload-performances
        path: tests/cicd/output/performances/*
